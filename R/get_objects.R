#' Get objects from the RATO ArcGIS Enterprise environment via the API
#'
#' This function retrieves objects from the RATO ArcGIS environment based on
#' the object_id. It is capable of retrieving a large amount of objects in a
#' single function call. As to minimize errors it will  fetch `batch size`
#' number of objects per request, and will operate in parallel: for a maximum of
#' 5000 requests per 60 seconds, having at most 10 requests open at a time.
#'
#' @param object_ids Vector of object_ids, will be converted to character.
#' @param token Access token, by default generated by `get_token()`
#' @param batch_size Number of objects to request per API call, default is 50.
#'   Setting this to a lower number will result in more API calls, but will also
#'   reduce the risk of timeouts or errors when requesting large datasets. When
#'   set to a higher number, the API may timeout or return an error if the
#'   dataset is too large.
#'
#' @return tibble of requested objects.
#' @export
get_objects <- function(object_ids, token = get_token(), batch_size = 50) {
  # Assert that objects were requested
  assertthat::assert_that(assertthat::not_empty(object_ids))

  # Assert that batch size is a positive integer
  assertthat::assert_that(assertthat::is.count(batch_size))

  # Assert that batch size is not too big
  default_batch_size <- 
    rlang::eval_bare(formals(rlang::caller_fn(0))[["batch_size"]])
  if (batch_size > default_batch_size) {
    warning(glue::glue(
      "Batch size is set to a higher than default value ",
      "this may result in timeouts or errors."
    ))
  }

  # Split requested object_ids into batches
  batched_ids <-
    split(object_ids, ceiling(seq_along(object_ids) / batch_size))

  # Build requests for the API
  objects_requests <-
    purrr::map(
      batched_ids,
      \(ids, token_to_use = token) {
        # Collate the object id's to query and format them as expected for the API
        object_id_query <- glue::glue(
          "OBJECTID IN ({object_ids_collated})",
          object_ids_collated = glue::glue_collapse(ids, sep = ",")
        )
        
      httr2::request("https://gis.oost-vlaanderen.be/server/rest/services/") %>%
        httr2::req_url_path_append(
          "RATO2",
          "RATO2_Dossiers_Publiek",
          "MapServer",
          "0",
          "query"
        ) %>%
        httr2::req_url_query(
          where = object_id_query,
          outFields = "*",
          f = "json",
          token = token_to_use
        )
    }) %>%
    # Set capacity of API: handle capacity outstanding requests, then wait for
    # requests to finish
    purrr::map(~ httr2::req_throttle(.x, capacity = 5000)) %>%
    # max_tries needs to be 2 for req_parallel
    purrr::map(~ httr2::req_retry(.x, max_tries = 2))

  # Parse the response
  objects_response <-
    objects_requests %>%
    httr2::req_perform_parallel(
      progress = "Fetching"
    ) %>%
    purrr::map(httr2::resp_body_json)

  # Forward any error messages
  assertthat::assert_that(
    all(purrr::map_lgl(objects_response, "error", "message", .default = TRUE)),
    msg = purrr::map(objects_response, "error", "message") %>%
      purrr::compact() %>%
      unique()
  )

  # Get the parts of the response we want

  objects_attr <- objects_response %>%
    purrr::map("features") %>%
    purrr::list_flatten() %>%
    purrr::map("attributes")

  # data.table is much faster than dplyr (purrr::list_rbind) for large list to
  # df conversion because it uses C internally.

  if (rlang::is_installed("data.table")) {
    objects_df <-
      # data.table will warn for fill (NULL to NA) even if set to TRUE
      suppressWarnings(data.table::rbindlist(objects_attr, fill = TRUE))
  } else {
    # data.table is not available, so fall-back on dplyr.
    objects_df <-
      # Convert every record in a single row data.frame, replace NULL with NA
      purrr::map(objects_attr, ~ as.data.frame(purrr::compact(.x))) %>%
      # Combine all the data.frames together so we have one row per record
      purrr::list_rbind()
  }

  return(objects_df)
}
